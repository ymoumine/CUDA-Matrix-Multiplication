{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "CJJb8CYIOQS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiJDWNUFHSdO",
        "outputId": "537990d1-62cf-4f1b-b372-1962f6045566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 16 14:59:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XT4gq7MH0Pu",
        "outputId": "33f5d76b-e308-41f0-fbf1-f5071e189741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.org (3.163.125.119)] [C\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.org (3.163.125.119)] [C\r                                                                                                    \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,110 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,482 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,425 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,164 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,616 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,701 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,453 kB]\n",
            "Fetched 20.3 MB in 5s (3,977 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNsmKocgH--X",
        "outputId": "1e3dc9cc-88bd-47f5-bf1d-3a80cf3820e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Some packages could not be installed. This may mean that you have\n",
            "requested an impossible situation or if you are using the unstable\n",
            "distribution that some required packages have not yet been created\n",
            "or been moved out of Incoming.\n",
            "The following information may help to resolve the situation:\n",
            "\n",
            "The following packages have unmet dependencies:\n",
            " nvidia-driver-565-open : Depends: libnvidia-gl-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: nvidia-dkms-565-open (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: nvidia-kernel-common-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: nvidia-kernel-source-565-open (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: libnvidia-compute-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: libnvidia-extra-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: nvidia-compute-utils-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: libnvidia-decode-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: libnvidia-encode-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: nvidia-utils-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: xserver-xorg-video-nvidia-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: libnvidia-cfg1-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Depends: libnvidia-fbc1-565 (= 565.57.01-0ubuntu1) but it is not installable\n",
            "                          Recommends: libnvidia-compute-565:i386 (= 565.57.01-0ubuntu1)\n",
            "                          Recommends: libnvidia-decode-565:i386 (= 565.57.01-0ubuntu1)\n",
            "                          Recommends: libnvidia-encode-565:i386 (= 565.57.01-0ubuntu1)\n",
            "                          Recommends: libnvidia-fbc1-565:i386 (= 565.57.01-0ubuntu1)\n",
            "                          Recommends: libnvidia-gl-565:i386 (= 565.57.01-0ubuntu1)\n",
            "E: Unable to correct problems, you have held broken packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Impl√©mentation Baseline"
      ],
      "metadata": {
        "id": "DpGnFzIaCV3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_global_baseline.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeUopccfIGeU",
        "outputId": "4c2ffc8d-171a-471d-f148-0f6fcd513568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrice_multiplication_global_baseline.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_global_baseline.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define N 512  // Taille des matrices carr√©es (N x N)\n",
        "\n",
        "__global__ void matrixMultiply(const float* A, const float* B, float* C, int n) {\n",
        "    // Obtenir l'indice de la ligne et de la colonne pour le thread actuel\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Calculer l'√©l√©ment de la matrice r√©sultat C\n",
        "    if (row < n && col < n) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            sum += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "    float *h_A, *h_B, *h_C;\n",
        "    float *d_A, *d_B, *d_C;\n",
        "\n",
        "    // Allocation de la m√©moire h√¥te\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B avec des valeurs arbitraires\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = 1.0f;  // exemple de valeurs\n",
        "        h_B[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Allocation de la m√©moire sur le p√©riph√©rique (GPU)\n",
        "    cudaMalloc((void**)&d_A, size);\n",
        "    cudaMalloc((void**)&d_B, size);\n",
        "    cudaMalloc((void**)&d_C, size);\n",
        "\n",
        "    // Copie des matrices A et B de l'h√¥te vers le p√©riph√©rique\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configuration de la grille et des blocs\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Lancer le noyau CUDA\n",
        "    matrixMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Copier le r√©sultat du p√©riph√©rique vers l'h√¥te\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // V√©rification du r√©sultat (optionnel)\n",
        "    std::cout << \"Exemple de sortie (C[0][0]): \" << h_C[0] << std::endl;\n",
        "\n",
        "    // Lib√©ration de la m√©moire\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBxsCgWaIM-9",
        "outputId": "c8324fba-7d34-4060-9783-bfe0d4ab9e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrice_multiplication_global_baseline.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrice_multiplication_global_baseline.cu -o matrice_multiplication_global_baseline"
      ],
      "metadata": {
        "id": "FrcmrePcJTlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrice_multiplication_global_baseline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzSAoZtUJyNu",
        "outputId": "684bc414-cb3a-4e36-d742-4a68c6cec8d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemple de sortie (C[0][0]): 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrice_multiplication_global_baseline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWymYssVdzhG",
        "outputId": "7aabc513-65e2-423a-a4fc-cb6dc9a7a06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==964== NVPROF is profiling process 964, command: ./matrice_multiplication_global_baseline\n",
            "Exemple de sortie (C[0][0]): 512\n",
            "==964== Profiling application: ./matrice_multiplication_global_baseline\n",
            "==964== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   81.57%  1.1357ms         1  1.1357ms  1.1357ms  1.1357ms  matrixMultiply(float const *, float const *, float*, int)\n",
            "                   12.56%  174.88us         2  87.438us  87.390us  87.486us  [CUDA memcpy HtoD]\n",
            "                    5.87%  81.694us         1  81.694us  81.694us  81.694us  [CUDA memcpy DtoH]\n",
            "      API calls:   95.71%  68.915ms         3  22.972ms  3.3660us  68.840ms  cudaMalloc\n",
            "                    3.48%  2.5036ms         3  834.54us  233.16us  1.9630ms  cudaMemcpy\n",
            "                    0.31%  223.13us         3  74.375us  12.598us  118.56us  cudaFree\n",
            "                    0.28%  201.05us         1  201.05us  201.05us  201.05us  cudaLaunchKernel\n",
            "                    0.19%  135.14us       114  1.1850us     146ns  54.194us  cuDeviceGetAttribute\n",
            "                    0.02%  11.499us         1  11.499us  11.499us  11.499us  cuDeviceGetName\n",
            "                    0.01%  5.1500us         1  5.1500us  5.1500us  5.1500us  cuDeviceGetPCIBusId\n",
            "                    0.01%  4.2410us         1  4.2410us  4.2410us  4.2410us  cuDeviceTotalMem\n",
            "                    0.00%  1.6370us         3     545ns     182ns  1.1900us  cuDeviceGetCount\n",
            "                    0.00%  1.1610us         2     580ns     175ns     986ns  cuDeviceGet\n",
            "                    0.00%     540ns         1     540ns     540ns     540ns  cuModuleGetLoadingMode\n",
            "                    0.00%     253ns         1     253ns     253ns     253ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Optimisation de l'acc√®s √† la m√©moire globale"
      ],
      "metadata": {
        "id": "kjf4IN3RCfvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_global_optimized.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e96876-fc0b-4c58-bdc2-7d1ca6109c40",
        "id": "JC2hCT8sF42t"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrice_multiplication_global_optimized.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_global_optimized.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define N 512  // Taille des matrices carr√©es (N x N)\n",
        "\n",
        "// Noyau CUDA pour la multiplication de matrices\n",
        "__global__ void matrixMultiply(const float* A, const float* B_T, float* C, int n) {\n",
        "    // Obtenir l'indice de la ligne et de la colonne pour le thread actuel\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Calculer l'√©l√©ment de la matrice r√©sultat C\n",
        "    if (row < n && col < n) {\n",
        "        float sum = 0.0f;\n",
        "        for (int k = 0; k < n; ++k) {\n",
        "            sum += A[row * n + k] * B_T[col * n + k]; // B_T est transpos√©e\n",
        "        }\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Fonction pour transposer une matrice sur le CPU\n",
        "void transposeMatrix(const float* src, float* dest, int n) {\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        for (int j = 0; j < n; ++j) {\n",
        "            dest[j * n + i] = src[i * n + j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float);\n",
        "    float *h_A, *h_B, *h_B_T, *h_C;\n",
        "    float *d_A, *d_B_T, *d_C;\n",
        "\n",
        "    // Allocation de la m√©moire h√¥te\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_B_T = (float*)malloc(size); // Pour stocker la matrice transpos√©e\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B avec des valeurs arbitraires\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = 1.0f;  // Exemple de valeurs\n",
        "        h_B[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Transposer la matrice B sur le CPU\n",
        "    transposeMatrix(h_B, h_B_T, N);\n",
        "\n",
        "    // Allocation de la m√©moire sur le p√©riph√©rique (GPU)\n",
        "    cudaMalloc((void**)&d_A, size);\n",
        "    cudaMalloc((void**)&d_B_T, size);\n",
        "    cudaMalloc((void**)&d_C, size);\n",
        "\n",
        "    // Copie des matrices A et B_T (transpos√©e) de l'h√¥te vers le p√©riph√©rique\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B_T, h_B_T, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configuration de la grille et des blocs\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Lancer le noyau CUDA\n",
        "    matrixMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B_T, d_C, N);\n",
        "\n",
        "    // Copier le r√©sultat du p√©riph√©rique vers l'h√¥te\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // V√©rification du r√©sultat (optionnel)\n",
        "    std::cout << \"Exemple de sortie (C[0][0]): \" << h_C[0] << std::endl;\n",
        "\n",
        "    // Lib√©ration de la m√©moire\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_B_T);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B_T);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c69d2d72-ead1-4369-8cd1-88ec68a06f44",
        "id": "UYcU5uZLF-QP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrice_multiplication_global_optimized.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrice_multiplication_global_optimized.cu -o matrice_multiplication_global_optimized"
      ],
      "metadata": {
        "id": "KIwFxMyiGHrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrice_multiplication_global_optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4246c8b-0855-4633-b07d-dbe229bec873",
        "id": "WLVftxGkGJr5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemple de sortie (C[0][0]): 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrice_multiplication_global_optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633f452d-b3d2-4ef6-81f1-e7f8aac18f09",
        "id": "yUYDwLmcGLsu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1185== NVPROF is profiling process 1185, command: ./matrice_multiplication_global_optimized\n",
            "Exemple de sortie (C[0][0]): 512\n",
            "==1185== Profiling application: ./matrice_multiplication_global_optimized\n",
            "==1185== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   95.92%  6.5611ms         1  6.5611ms  6.5611ms  6.5611ms  matrixMultiply(float const *, float const *, float*, int)\n",
            "                    2.56%  175.39us         2  87.694us  87.614us  87.774us  [CUDA memcpy HtoD]\n",
            "                    1.52%  104.00us         1  104.00us  104.00us  104.00us  [CUDA memcpy DtoH]\n",
            "      API calls:   91.55%  92.703ms         3  30.901ms  7.0670us  92.624ms  cudaMalloc\n",
            "                    7.85%  7.9501ms         3  2.6500ms  237.32us  7.4050ms  cudaMemcpy\n",
            "                    0.23%  232.00us         3  77.334us  14.623us  119.76us  cudaFree\n",
            "                    0.20%  202.60us         1  202.60us  202.60us  202.60us  cudaLaunchKernel\n",
            "                    0.14%  142.33us       114  1.2480us     138ns  57.448us  cuDeviceGetAttribute\n",
            "                    0.01%  12.186us         1  12.186us  12.186us  12.186us  cuDeviceGetName\n",
            "                    0.01%  5.5180us         1  5.5180us  5.5180us  5.5180us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.5050us         1  4.5050us  4.5050us  4.5050us  cuDeviceTotalMem\n",
            "                    0.00%  1.7160us         3     572ns     203ns  1.1550us  cuDeviceGetCount\n",
            "                    0.00%  1.2800us         2     640ns     235ns  1.0450us  cuDeviceGet\n",
            "                    0.00%     491ns         1     491ns     491ns     491ns  cuModuleGetLoadingMode\n",
            "                    0.00%     235ns         1     235ns     235ns     235ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Utilisation de la m√©moire partag√©e"
      ],
      "metadata": {
        "id": "TWGDuV0PCl5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_shared.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07eef928-809d-41f7-b3f5-32883b39346e",
        "id": "7KRRIThO5F4K"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrice_multiplication_shared.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_shared.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define TILE_SIZE 16  // Taille de la tuile (TILE_SIZE x TILE_SIZE)\n",
        "#define N 512         // Taille des matrices carr√©es (N x N)\n",
        "\n",
        "// Noyau CUDA pour la multiplication matricielle optimis√©e avec m√©moire partag√©e\n",
        "__global__ void matrixMultiplyShared(const float* A, const float* B, float* C, int n) {\n",
        "    // D√©claration de m√©moire partag√©e pour stocker les tuiles de A et B\n",
        "    __shared__ float tile_A[TILE_SIZE][TILE_SIZE];\n",
        "    __shared__ float tile_B[TILE_SIZE][TILE_SIZE];\n",
        "\n",
        "    // Calculer les indices de ligne et de colonne pour le thread actuel\n",
        "    int row = blockIdx.y * TILE_SIZE + threadIdx.y; // Ligne g√©r√©e par ce thread\n",
        "    int col = blockIdx.x * TILE_SIZE + threadIdx.x; // Colonne g√©r√©e par ce thread\n",
        "    float sum = 0.0f; // Accumulateur pour le produit-somme\n",
        "\n",
        "    // Boucle sur toutes les tuiles n√©cessaires\n",
        "    for (int tileIdx = 0; tileIdx < (n + TILE_SIZE - 1) / TILE_SIZE; ++tileIdx) {\n",
        "        // Charger une tuile de A dans la m√©moire partag√©e\n",
        "        if (row < n && (tileIdx * TILE_SIZE + threadIdx.x) < n)\n",
        "            tile_A[threadIdx.y][threadIdx.x] = A[row * n + tileIdx * TILE_SIZE + threadIdx.x];\n",
        "        else\n",
        "            tile_A[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Charger une tuile de B dans la m√©moire partag√©e\n",
        "        if (col < n && (tileIdx * TILE_SIZE + threadIdx.y) < n)\n",
        "            tile_B[threadIdx.y][threadIdx.x] = B[(tileIdx * TILE_SIZE + threadIdx.y) * n + col];\n",
        "        else\n",
        "            tile_B[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Synchroniser les threads pour s'assurer que la tuile est compl√®tement charg√©e\n",
        "        __syncthreads();\n",
        "\n",
        "        // Calcul produit-somme sur la tuile actuelle\n",
        "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
        "            sum += tile_A[threadIdx.y][k] * tile_B[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        // Synchroniser les threads avant de charger la prochaine tuile\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Stocker le r√©sultat final dans la m√©moire globale\n",
        "    if (row < n && col < n) {\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float); // Taille totale en octets des matrices\n",
        "    float *h_A, *h_B, *h_C;          // Pointeurs pour la m√©moire h√¥te\n",
        "    float *d_A, *d_B, *d_C;          // Pointeurs pour la m√©moire p√©riph√©rique (GPU)\n",
        "\n",
        "    // Allocation de m√©moire sur l'h√¥te\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = 1.0f; // Chaque √©l√©ment de A est 1.0\n",
        "        h_B[i] = 1.0f; // Chaque √©l√©ment de B est 1.0\n",
        "    }\n",
        "\n",
        "    // Allocation de m√©moire sur le GPU\n",
        "    cudaMalloc((void**)&d_A, size);\n",
        "    cudaMalloc((void**)&d_B, size);\n",
        "    cudaMalloc((void**)&d_C, size);\n",
        "\n",
        "    // Copier les matrices A et B depuis l'h√¥te vers le GPU\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configuration des dimensions des threads et des blocs\n",
        "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE); // Chaque bloc contient TILE_SIZE x TILE_SIZE threads\n",
        "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE,\n",
        "                       (N + TILE_SIZE - 1) / TILE_SIZE); // Nombre de blocs n√©cessaires\n",
        "\n",
        "    // Lancer le noyau CUDA\n",
        "    matrixMultiplyShared<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copier le r√©sultat depuis le GPU vers la m√©moire h√¥te\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Afficher un exemple de r√©sultat pour validation\n",
        "    std::cout << \"C[0][0] (Shared): \" << h_C[0] << std::endl;\n",
        "\n",
        "    // Lib√©ration de m√©moire sur l'h√¥te et le GPU\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1o_bkmeeRTe",
        "outputId": "0bcbd9d5-51f9-4eb7-fffb-6a8cf8897336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrice_multiplication_shared.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrice_multiplication_shared.cu -o matrix_multiplication_shared"
      ],
      "metadata": {
        "id": "48W4CpsFdkPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_multiplication_shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoRLJ7fKdwJ8",
        "outputId": "c44fb2bd-9b6d-4d5d-98bb-18fbae0df5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C[0][0] (Shared): 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrix_multiplication_shared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e60b57c-138b-4aad-bd41-3a3a556da47b",
        "id": "KNds6xhF4C9L"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1428== NVPROF is profiling process 1428, command: ./matrix_multiplication_shared\n",
            "C[0][0] (Shared): 512\n",
            "==1428== Profiling application: ./matrix_multiplication_shared\n",
            "==1428== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   73.31%  742.00us         1  742.00us  742.00us  742.00us  matrixMultiplyShared(float const *, float const *, float*, int)\n",
            "                   17.28%  174.88us         2  87.437us  87.262us  87.613us  [CUDA memcpy HtoD]\n",
            "                    9.41%  95.294us         1  95.294us  95.294us  95.294us  [CUDA memcpy DtoH]\n",
            "      API calls:   96.35%  72.931ms         3  24.310ms  3.7850us  72.849ms  cudaMalloc\n",
            "                    1.87%  1.4174ms         3  472.48us  252.28us  844.49us  cudaMemcpy\n",
            "                    0.98%  745.13us         1  745.13us  745.13us  745.13us  cudaDeviceSynchronize\n",
            "                    0.30%  228.87us         3  76.291us  13.476us  119.75us  cudaFree\n",
            "                    0.28%  212.62us         1  212.62us  212.62us  212.62us  cudaLaunchKernel\n",
            "                    0.18%  135.60us       114  1.1890us     146ns  53.092us  cuDeviceGetAttribute\n",
            "                    0.02%  11.448us         1  11.448us  11.448us  11.448us  cuDeviceGetName\n",
            "                    0.01%  5.2480us         1  5.2480us  5.2480us  5.2480us  cuDeviceGetPCIBusId\n",
            "                    0.01%  4.6040us         1  4.6040us  4.6040us  4.6040us  cuDeviceTotalMem\n",
            "                    0.00%  1.3720us         3     457ns     204ns     920ns  cuDeviceGetCount\n",
            "                    0.00%     869ns         2     434ns     189ns     680ns  cuDeviceGet\n",
            "                    0.00%     490ns         1     490ns     490ns     490ns  cuModuleGetLoadingMode\n",
            "                    0.00%     238ns         1     238ns     238ns     238ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Optimiser les sch√©mas d'acc√®s √† la m√©moire partag√©e"
      ],
      "metadata": {
        "id": "RTXrb5cNCsCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_shared_optimized.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16sTnZKyApVV",
        "outputId": "32f3858b-b6ef-4acb-dddd-1ff53c30d5b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrice_multiplication_shared_optimized.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_shared_optimized.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define TILE_SIZE 16  // Taille de la tuile (TILE_SIZE x TILE_SIZE)\n",
        "#define PADDING 1         // Padding pour √©viter les conflits de banque\n",
        "#define N 512         // Taille des matrices carr√©es (N x N)\n",
        "\n",
        "// Noyau CUDA pour la multiplication matricielle optimis√©e avec m√©moire partag√©e et padding\n",
        "__global__ void matrixMultiplySharedOptimized(const float* A, const float* B, float* C, int n) {\n",
        "    // D√©claration de m√©moire partag√©e avec padding\n",
        "    __shared__ float tile_A[TILE_SIZE][TILE_SIZE + PADDING];\n",
        "    __shared__ float tile_B[TILE_SIZE][TILE_SIZE + PADDING];\n",
        "\n",
        "    // Calculer les indices de ligne et de colonne pour le thread actuel\n",
        "    int row = blockIdx.y * TILE_SIZE + threadIdx.y; // Ligne g√©r√©e par ce thread\n",
        "    int col = blockIdx.x * TILE_SIZE + threadIdx.x; // Colonne g√©r√©e par ce thread\n",
        "    float sum = 0.0f; // Accumulateur pour le produit-somme\n",
        "\n",
        "    // Boucle sur toutes les tuiles n√©cessaires\n",
        "    for (int tileIdx = 0; tileIdx < (n + TILE_SIZE - 1) / TILE_SIZE; ++tileIdx) {\n",
        "        // Charger une tuile de A dans la m√©moire partag√©e avec padding\n",
        "        if (row < n && (tileIdx * TILE_SIZE + threadIdx.x) < n)\n",
        "            tile_A[threadIdx.y][threadIdx.x] = A[row * n + tileIdx * TILE_SIZE + threadIdx.x];\n",
        "        else\n",
        "            tile_A[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Charger une tuile de B dans la m√©moire partag√©e avec padding\n",
        "        if (col < n && (tileIdx * TILE_SIZE + threadIdx.y) < n)\n",
        "            tile_B[threadIdx.y][threadIdx.x] = B[(tileIdx * TILE_SIZE + threadIdx.y) * n + col];\n",
        "        else\n",
        "            tile_B[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Synchroniser les threads pour s'assurer que la tuile est compl√®tement charg√©e\n",
        "        __syncthreads();\n",
        "\n",
        "        // Calcul produit-somme sur la tuile actuelle\n",
        "        for (int k = 0; k < TILE_SIZE; ++k) {\n",
        "            sum += tile_A[threadIdx.y][k] * tile_B[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        // Synchroniser les threads avant de charger la prochaine tuile\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Stocker le r√©sultat final dans la m√©moire globale\n",
        "    if (row < n && col < n) {\n",
        "        C[row * n + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(float); // Taille totale en octets des matrices\n",
        "    float *h_A, *h_B, *h_C;          // Pointeurs pour la m√©moire h√¥te\n",
        "    float *d_A, *d_B, *d_C;          // Pointeurs pour la m√©moire p√©riph√©rique (GPU)\n",
        "\n",
        "    // Allocation de m√©moire sur l'h√¥te\n",
        "    h_A = (float*)malloc(size);\n",
        "    h_B = (float*)malloc(size);\n",
        "    h_C = (float*)malloc(size);\n",
        "\n",
        "    // Initialisation des matrices A et B\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        h_A[i] = 1.0f; // Chaque √©l√©ment de A est 1.0\n",
        "        h_B[i] = 1.0f; // Chaque √©l√©ment de B est 1.0\n",
        "    }\n",
        "\n",
        "    // Allocation de m√©moire sur le GPU\n",
        "    cudaMalloc((void**)&d_A, size);\n",
        "    cudaMalloc((void**)&d_B, size);\n",
        "    cudaMalloc((void**)&d_C, size);\n",
        "\n",
        "    // Copier les matrices A et B depuis l'h√¥te vers le GPU\n",
        "    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configuration des dimensions des threads et des blocs\n",
        "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE); // Chaque bloc contient TILE_SIZE x TILE_SIZE threads\n",
        "    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE,\n",
        "                       (N + TILE_SIZE - 1) / TILE_SIZE); // Nombre de blocs n√©cessaires\n",
        "\n",
        "    // Lancer le noyau CUDA optimis√©\n",
        "    matrixMultiplySharedOptimized<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copier le r√©sultat depuis le GPU vers la m√©moire h√¥te\n",
        "    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Afficher un exemple de r√©sultat pour validation\n",
        "    std::cout << \"C[0][0] (Shared Optimized): \" << h_C[0] << std::endl;\n",
        "\n",
        "    // Lib√©ration de m√©moire sur l'h√¥te et le GPU\n",
        "    free(h_A);\n",
        "    free(h_B);\n",
        "    free(h_C);\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5bab50-25ef-4efc-b311-1a0a2284245b",
        "id": "MOVHGlmjGeu6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrice_multiplication_shared_optimized.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrice_multiplication_shared_optimized.cu -o matrix_multiplication_shared_optimized"
      ],
      "metadata": {
        "id": "5TCWpmjdGky-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix_multiplication_shared_optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30a739f9-f05b-4d61-8fc8-68afb5edd82c",
        "id": "6ekk8u-bG1l9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C[0][0] (Shared Optimized): 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrix_multiplication_shared_optimized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7716bd-99fe-41b1-f1d5-38bb602160d0",
        "id": "ldEl_QjdG4Zm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1621== NVPROF is profiling process 1621, command: ./matrix_multiplication_shared_optimized\n",
            "C[0][0] (Shared Optimized): 512\n",
            "==1621== Profiling application: ./matrix_multiplication_shared_optimized\n",
            "==1621== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   78.45%  975.69us         1  975.69us  975.69us  975.69us  matrixMultiplySharedOptimized(float const *, float const *, float*, int)\n",
            "                   14.04%  174.65us         2  87.326us  87.294us  87.358us  [CUDA memcpy HtoD]\n",
            "                    7.51%  93.438us         1  93.438us  93.438us  93.438us  [CUDA memcpy DtoH]\n",
            "      API calls:   96.65%  90.084ms         3  30.028ms  6.2770us  90.007ms  cudaMalloc\n",
            "                    1.63%  1.5182ms         3  506.07us  254.94us  948.80us  cudaMemcpy\n",
            "                    1.05%  978.32us         1  978.32us  978.32us  978.32us  cudaDeviceSynchronize\n",
            "                    0.26%  240.11us         3  80.037us  14.679us  124.65us  cudaFree\n",
            "                    0.22%  208.38us         1  208.38us  208.38us  208.38us  cudaLaunchKernel\n",
            "                    0.16%  146.46us       114  1.2840us     146ns  56.985us  cuDeviceGetAttribute\n",
            "                    0.01%  12.311us         1  12.311us  12.311us  12.311us  cuDeviceGetName\n",
            "                    0.01%  6.0520us         1  6.0520us  6.0520us  6.0520us  cuDeviceTotalMem\n",
            "                    0.01%  5.0730us         1  5.0730us  5.0730us  5.0730us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4820us         3     494ns     184ns     930ns  cuDeviceGetCount\n",
            "                    0.00%  1.0280us         2     514ns     311ns     717ns  cuDeviceGet\n",
            "                    0.00%     349ns         1     349ns     349ns     349ns  cuModuleGetLoadingMode\n",
            "                    0.00%     338ns         1     338ns     338ns     338ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Profilage des performances"
      ],
      "metadata": {
        "id": "YBxt_S5aCzie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_profiling.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpxnncXIF_-l",
        "outputId": "8dd4a1ad-555c-4c98-951b-4b3a2461c2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrice_multiplication_profiling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_profiling.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "// Noyau pour la multiplication matricielle utilisant la m√©moire globale\n",
        "__global__ void matMulGlobalMemory(float *A, float *B, float *C, int N) {\n",
        "    // Calcul des indices de ligne et de colonne pour chaque thread\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // V√©rifier si les indices sont dans les limites de la matrice\n",
        "    if (row < N && col < N) {\n",
        "        float value = 0.0f; // Variable pour accumuler le produit-somme\n",
        "        // Calculer la multiplication matricielle pour cet √©l√©ment\n",
        "        for (int k = 0; k < N; ++k) {\n",
        "            value += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        C[row * N + col] = value; // Stocker le r√©sultat dans C\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1024; // Dimension de la matrice (N x N)\n",
        "    // Allouer des matrices A, B, et C sur l'h√¥te (CPU)\n",
        "    std::vector<float> h_A(N * N, 1.0f); // Matrice A (initialis√©e √† 1)\n",
        "    std::vector<float> h_B(N * N, 1.0f); // Matrice B (initialis√©e √† 1)\n",
        "    std::vector<float> h_C(N * N, 0.0f); // Matrice C (initialis√©e √† 0)\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    // Allocation m√©moire sur le GPU pour A, B et C\n",
        "    cudaMalloc(&d_A, N * N * sizeof(float));\n",
        "    cudaMalloc(&d_B, N * N * sizeof(float));\n",
        "    cudaMalloc(&d_C, N * N * sizeof(float));\n",
        "\n",
        "    // Copier les matrices A et B depuis l'h√¥te vers le GPU\n",
        "    cudaMemcpy(d_A, h_A.data(), N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B.data(), N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // D√©finir les dimensions des blocs et de la grille\n",
        "    dim3 dimBlock(16, 16, 1);  // Taille du bloc de threads\n",
        "    dim3 dimGrid((N + 15) / 16, (N + 15) / 16, 1); // Nombre de blocs n√©cessaires\n",
        "\n",
        "    // Lancer le noyau pour effectuer la multiplication matricielle\n",
        "    matMulGlobalMemory<<<dimGrid, dimBlock>>>(d_A, d_B, d_C, N);\n",
        "    cudaDeviceSynchronize(); // Synchroniser l'ex√©cution du noyau\n",
        "\n",
        "    // Copier le r√©sultat de la matrice C du GPU vers l'h√¥te\n",
        "    cudaMemcpy(h_C.data(), d_C, N * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Afficher un exemple de r√©sultat pour v√©rifier\n",
        "    std::cout << \"Exemple de sortie (C[0][0]): \" << h_C[0] << std::endl;\n",
        "\n",
        "    // Lib√©rer la m√©moire allou√©e sur le GPU\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2MUiiq0LrF2",
        "outputId": "45e3fac9-a231-4c96-acfe-e285e562402a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrice_multiplication_profiling.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrice_multiplication_profiling.cu -o matrice_multiplication_profiling"
      ],
      "metadata": {
        "id": "KVT0LXy-MyOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrice_multiplication_profiling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sb8koHRMzKD",
        "outputId": "33bc3b0f-b31a-4293-9a2f-ec0f30f8d33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemple de sortie (C[0][0]): 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrice_multiplication_profiling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4SyNhwNM1mk",
        "outputId": "ebb1cdc1-2b1b-40c2-f109-522192d594b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1804== NVPROF is profiling process 1804, command: ./matrice_multiplication_profiling\n",
            "Exemple de sortie (C[0][0]): 1024\n",
            "==1804== Profiling application: ./matrice_multiplication_profiling\n",
            "==1804== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   80.66%  9.1159ms         1  9.1159ms  9.1159ms  9.1159ms  matMulGlobalMemory(float*, float*, float*, int)\n",
            "                   13.68%  1.5458ms         2  772.88us  760.17us  785.58us  [CUDA memcpy HtoD]\n",
            "                    5.66%  639.67us         1  639.67us  639.67us  639.67us  [CUDA memcpy DtoH]\n",
            "      API calls:   86.55%  84.065ms         3  28.022ms  129.78us  83.787ms  cudaMalloc\n",
            "                    9.39%  9.1212ms         1  9.1212ms  9.1212ms  9.1212ms  cudaDeviceSynchronize\n",
            "                    3.07%  2.9787ms         3  992.90us  967.81us  1.0330ms  cudaMemcpy\n",
            "                    0.55%  535.58us         3  178.53us  117.49us  210.81us  cudaFree\n",
            "                    0.27%  262.96us         1  262.96us  262.96us  262.96us  cudaLaunchKernel\n",
            "                    0.14%  138.59us       114  1.2150us     135ns  56.111us  cuDeviceGetAttribute\n",
            "                    0.01%  10.475us         1  10.475us  10.475us  10.475us  cuDeviceGetName\n",
            "                    0.01%  8.7790us         1  8.7790us  8.7790us  8.7790us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.5190us         1  4.5190us  4.5190us  4.5190us  cuDeviceTotalMem\n",
            "                    0.00%  1.8230us         3     607ns     225ns  1.3580us  cuDeviceGetCount\n",
            "                    0.00%     977ns         2     488ns     184ns     793ns  cuDeviceGet\n",
            "                    0.00%     576ns         1     576ns     576ns     576ns  cuModuleGetLoadingMode\n",
            "                    0.00%     238ns         1     238ns     238ns     238ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Optimisation it√©rative\n",
        "\n"
      ],
      "metadata": {
        "id": "vvlxeeA_EGLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_iterative_optimization.cu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDNyeMBNLu4Z",
        "outputId": "862e3f46-ada2-42ee-91ac-64b31a96f639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrice_multiplication_iterative_optimization.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrice_multiplication_iterative_optimization.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "#define TILE_WIDTH 16 // Taille de la tuile pour la multiplication optimis√©e\n",
        "\n",
        "// Noyau CUDA pour la multiplication matricielle utilisant la m√©moire partag√©e\n",
        "__global__ void matMulSharedMemory(float *A, float *B, float *C, int N) {\n",
        "    // D√©claration de la m√©moire partag√©e pour les tuiles de A et B\n",
        "    __shared__ float tileA[TILE_WIDTH][TILE_WIDTH];\n",
        "    __shared__ float tileB[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "    // Calcul des indices de ligne et de colonne pour chaque thread\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    float value = 0.0f; // Variable pour accumuler le produit-somme\n",
        "\n",
        "    // Boucle sur les tuiles n√©cessaires pour la multiplication\n",
        "    for (int t = 0; t < (N + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
        "        // Charger une tuile de la matrice A dans la m√©moire partag√©e\n",
        "        if (row < N && t * TILE_WIDTH + threadIdx.x < N)\n",
        "            tileA[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_WIDTH + threadIdx.x];\n",
        "        else\n",
        "            tileA[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Charger une tuile de la matrice B dans la m√©moire partag√©e\n",
        "        if (col < N && t * TILE_WIDTH + threadIdx.y < N)\n",
        "            tileB[threadIdx.y][threadIdx.x] = B[(t * TILE_WIDTH + threadIdx.y) * N + col];\n",
        "        else\n",
        "            tileB[threadIdx.y][threadIdx.x] = 0.0f;\n",
        "\n",
        "        // Synchronisation des threads avant le calcul du produit-somme\n",
        "        __syncthreads();\n",
        "\n",
        "        // LE RESTE DU CODE...\n",
        "\n",
        "        // Calculer le produit-somme pour la tuile actuelle\n",
        "        for (int k = 0; k < TILE_WIDTH; ++k) {\n",
        "            value += tileA[threadIdx.y][k] * tileB[k][threadIdx.x];\n",
        "        }\n",
        "\n",
        "        // Synchroniser les threads avant de charger la prochaine tuile\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Stocker le r√©sultat final dans la matrice C\n",
        "    if (row < N && col < N) {\n",
        "        C[row * N + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1024; // Dimension de la matrice (N x N)\n",
        "    // Allouer des matrices A, B, et C sur l'h√¥te (CPU)\n",
        "    std::vector<float> h_A(N * N, 1.0f); // Matrice A (initialis√©e √† 1)\n",
        "    std::vector<float> h_B(N * N, 1.0f); // Matrice B (initialis√©e √† 1)\n",
        "    std::vector<float> h_C(N * N, 0.0f); // Matrice C (initialis√©e √† 0)\n",
        "\n",
        "    float *d_A, *d_B, *d_C;\n",
        "    // Allocation m√©moire sur le GPU pour A, B et C\n",
        "    cudaMalloc(&d_A, N * N * sizeof(float));\n",
        "    cudaMalloc(&d_B, N * N * sizeof(float));\n",
        "    cudaMalloc(&d_C, N * N * sizeof(float));\n",
        "\n",
        "    // Copier les matrices A et B depuis l'h√¥te vers le GPU\n",
        "    cudaMemcpy(d_A, h_A.data(), N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, h_B.data(), N * N * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // D√©finir les dimensions des blocs et de la grille\n",
        "    dim3 dimBlock(TILE_WIDTH, TILE_WIDTH, 1);  // Taille du bloc de threads\n",
        "    dim3 dimGrid((N + TILE_WIDTH - 1) / TILE_WIDTH, (N + TILE_WIDTH - 1) / TILE_WIDTH, 1); // Nombre de blocs n√©cessaires\n",
        "\n",
        "    // Lancer le noyau pour effectuer la multiplication matricielle optimis√©e\n",
        "    matMulSharedMemory<<<dimGrid, dimBlock>>>(d_A, d_B, d_C, N);\n",
        "    cudaDeviceSynchronize(); // Synchroniser l'ex√©cution du noyau\n",
        "\n",
        "    // Copier le r√©sultat de la matrice C du GPU vers l'h√¥te\n",
        "    cudaMemcpy(h_C.data(), d_C, N * N * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Afficher un exemple de r√©sultat pour v√©rifier\n",
        "    std::cout << \"Exemple de sortie (C[0][0]): \" << h_C[0] << std::endl;\n",
        "\n",
        "    // Lib√©rer la m√©moire allou√©e sur le GPU\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVTcsdEUL7JX",
        "outputId": "dfa9ce87-a8dd-4f18-a0bd-638c7723be35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matrice_multiplication_iterative_optimization.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc matrice_multiplication_iterative_optimization.cu -o matrice_multiplication_iterative_optimization"
      ],
      "metadata": {
        "id": "oMq6iPj2NAeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrice_multiplication_iterative_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS4CmAqcNBmQ",
        "outputId": "4618ea8f-4144-4251-f25b-09a9c06ff63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exemple de sortie (C[0][0]): 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./matrice_multiplication_iterative_optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMWknrz7NDlx",
        "outputId": "043c3d7c-3cc0-4a89-c116-428ad5e39c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1981== NVPROF is profiling process 1981, command: ./matrice_multiplication_iterative_optimization\n",
            "Exemple de sortie (C[0][0]): 1024\n",
            "==1981== Profiling application: ./matrice_multiplication_iterative_optimization\n",
            "==1981== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   73.09%  5.8026ms         1  5.8026ms  5.8026ms  5.8026ms  matMulSharedMemory(float*, float*, float*, int)\n",
            "                   18.51%  1.4696ms         2  734.82us  732.30us  737.33us  [CUDA memcpy HtoD]\n",
            "                    8.40%  667.09us         1  667.09us  667.09us  667.09us  [CUDA memcpy DtoH]\n",
            "      API calls:   89.99%  87.754ms         3  29.251ms  73.359us  87.602ms  cudaMalloc\n",
            "                    5.95%  5.8050ms         1  5.8050ms  5.8050ms  5.8050ms  cudaDeviceSynchronize\n",
            "                    3.03%  2.9536ms         3  984.54us  916.37us  1.0683ms  cudaMemcpy\n",
            "                    0.56%  542.35us         3  180.78us  126.62us  208.66us  cudaFree\n",
            "                    0.26%  251.15us         1  251.15us  251.15us  251.15us  cudaLaunchKernel\n",
            "                    0.19%  187.51us       114  1.6440us     137ns  96.130us  cuDeviceGetAttribute\n",
            "                    0.01%  11.332us         1  11.332us  11.332us  11.332us  cuDeviceGetName\n",
            "                    0.01%  5.5640us         1  5.5640us  5.5640us  5.5640us  cuDeviceGetPCIBusId\n",
            "                    0.01%  4.9160us         1  4.9160us  4.9160us  4.9160us  cuDeviceTotalMem\n",
            "                    0.00%  1.5690us         3     523ns     194ns  1.0700us  cuDeviceGetCount\n",
            "                    0.00%     955ns         2     477ns     209ns     746ns  cuDeviceGet\n",
            "                    0.00%     544ns         1     544ns     544ns     544ns  cuModuleGetLoadingMode\n",
            "                    0.00%     318ns         1     318ns     318ns     318ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ]
}